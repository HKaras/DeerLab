{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Selecting an optimal parametric model for fitting a dipolar signal\n\nHow to optimally select a parametric model for a given dipolar signal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport deerlab as dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generation\n\nLet's start by constructing a simple dipolar signal with some noise arising \nfrom a bimodal Gaussian distance distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Prepare the signal components\nt = np.linspace(-0.3,3.5,300)                # time axis, \u00b5s\nr = np.linspace(2,6,200)                     # distance axis, nm\nP = dl.dd_gauss2(r,[3.8, 0.4, 0.7, 4.5, 0.2, 0.7])   # distance distribution\nK = dl.dipolarkernel(t,r)                    # dipolar kernel matrix\nV = K@P + dl.whitegaussnoise(t,0.02)         # DEER signal, with added noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting an optimal model\n\nEven though we know the ground truth, in this example we will cosider the \nfollowing set of potential parametric models: \n\n* Unimodal Rician distribution\n* Bimodal Rician distribution\n* Trimodal Rician distribution\n* Unimodal Gaussian distribution\n* Bimodal Gaussian distribution\n* Trimodal Gaussian distribution\n* Mixed bimodal Gaussian/Rician distribution\n\nThe first six models have built-in parametric models which we can use directly. \nThe last model we can construct from built-in models using the ``mixmodels`` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Prepare the mixed model\ndd_rice_gauss = dl.mixmodels(dl.dd_rice,dl.dd_gauss)\n \n# Prepare list of candidate parametric models\nmodels = [dl.dd_rice,dl.dd_rice2,dl.dd_rice3,dl.dd_gauss,dl.dd_gauss2,dl.dd_gauss3,dd_rice_gauss]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to make an appropiate choice, we need some liklihood estimator. All fit functions is DeerLab returns a stats \ndictionary which contains (amongst other estimators) likelihood estimators such as the Akaike information criterion (AIC).\nThe model with the lowers AIC value can be considered to most likely to be the optimal model.\n\nTo do this, we just have to evaluate the parametric models with ``fitparamodel`` while looping over all the distribution models\nwe listed above, and collecting the AIC-values for each model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aic = []\nfor model in models:\n    info = model()\n    # Prepare the signal model with the new distance model\n    Vmodel = lambda par: K@model(r,par)\n    # Fit the signal\n    fit = dl.fitparamodel(V,Vmodel,par0=info['Start'],lb=info['Lower'],ub=info['Upper'])\n    parfit = fit.param\n    stats= fit.stats\n    # Add current AIC value to the list\n    aic.append(stats['aic'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the absolute AIC values have no meaning, it is standard practice to look at the relative \nchanges in AIC values between the evaluated models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "daic = aic - min(aic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Akaike Weights\n It is often more useful to look at these results from the perspective of\n Akaike weights, i.e. the probabilities of a model being the most optimal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = 100*np.exp(-(daic/2))/sum(np.exp(-daic/2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,8))\n\nplt.subplot(2,2,1)\nplt.plot(t,V,'k.')\nplt.grid(alpha=0.2)\nplt.xlabel('t [\u00b5s]')\nplt.legend(['data'])\n\nplt.subplot(2,2,2)\nplt.plot(r,P,'k',linewidth=1.5)\nplt.xlabel('r [nm]')\nplt.ylabel('P(r) [nm$^{-1}$]')\nplt.legend(['Ground truth'])\nplt.grid(alpha=0.2)\n\nmodelnames = [model.__name__ for model in models]\n\nplt.subplot(2,2,3)\nplt.bar(modelnames,daic,color='b',alpha=0.5)\nplt.ylabel('$\\Delta$AIC')\nplt.grid(alpha=0.2)\nplt.xticks(rotation=45)\n\n# Plot the results\nplt.subplot(2,2,4)\nplt.bar(modelnames,weights,color='b',alpha=0.5)\nplt.ylabel('Akaike Weights [%]')\nplt.xticks(rotation=45)\nplt.grid(alpha=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Typically there is not a single optimal model unless the noise level is very\nlow. Usually several models have similar probabilities and should therefore be presented together. \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}